{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给 storage 下的数据重新命名\n",
    "\n",
    "规则：\n",
    "\n",
    "```\n",
    "<dataset_id>_<小写的名字>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../../../storage'\n",
    "# count = 0\n",
    "# for file in os.listdir(data_root):\n",
    "#     try:\n",
    "#         dataset_id = file.split('_')[0]\n",
    "#         dataset_name = file.split('_')[1].lower()\n",
    "#         # print(dataset_name.lower())\n",
    "#     except Exception as e:\n",
    "#         dataset_name = file.replace(' ', '-').lower()\n",
    "    \n",
    "#     filename = f'{count}_{dataset_name}'\n",
    "#     origin_path = os.path.join(data_root, file)\n",
    "#     target_path = os.path.join(data_root, filename)\n",
    "#     os.rename(origin_path, target_path)\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据集\n",
    "\n",
    "1. 删除所有的 `._` 开头的文件夹\n",
    "2. 删除所有的空文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(dataset: str):\n",
    "    for file in os.listdir(dataset):\n",
    "        filepath = os.path.join(dataset, file)\n",
    "        if file == '.DS_Store' or (file.startswith('._')):\n",
    "            os.remove(filepath)\n",
    "        \n",
    "    for dirpath, dirnames, filenames in os.walk(dataset):\n",
    "        if not dirnames and not filenames:\n",
    "            shutil.rmtree(dirpath)\n",
    "            print('delete ' + dirpath)\n",
    "\n",
    "for file in os.listdir(data_root):\n",
    "    file_path = os.path.join(data_root, file)\n",
    "    data_id, data_name = file.split('_')\n",
    "    explore_dataset(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成每个数据集的元信息\n",
    "\n",
    "基础格式 参考 `./data/demo.json`\n",
    "\n",
    "- task_ids 映射表 `./data/task.map.json`\n",
    "- modality_ids 映射表 `./data/modality.map.json`\n",
    "- organ_ids 映射表 `./data/organ.map.json`\n",
    "\n",
    "基本策略：\n",
    "1. 先找 dataset.json\n",
    "2. 再找 含有 imagesTr, labelsTr 这样的字眼的文件夹\n",
    "3. 制定规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "task_map: dict = json.load(open('../data/task.map.json', 'r'))\n",
    "modality_map: dict = json.load(open('../data/modality.map.json', 'r'))\n",
    "organ_map: dict = json.load(open('../data/organ.map.json', 'r'))\n",
    "\n",
    "modality_name2id = { modality_map[id]: int(id) for id in modality_map }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modality_name2id['ct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data_num\": -1,\n",
      "    \"description\": \"hello world\",\n",
      "    \"id\": -1,\n",
      "    \"label_num\": -1,\n",
      "    \"modality_ids\": [],\n",
      "    \"name\": \"test\",\n",
      "    \"organ_ids\": [],\n",
      "    \"orgin_url\": \"\",\n",
      "    \"release_date\": \"\",\n",
      "    \"split_info\": {\n",
      "        \"train\": {\n",
      "            \"data\": 0,\n",
      "            \"label\": 0\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"data\": 0,\n",
      "            \"label\": 0\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"data\": 0,\n",
      "            \"label\": 0\n",
      "        }\n",
      "    },\n",
      "    \"task_ids\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class MedicalDataMeta:\n",
    "    def __init__(self, id, name) -> None:\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.orgin_url = ''\n",
    "        self.description = ''\n",
    "        self.release_date = ''\n",
    "        self.task_ids = []\n",
    "        self.modality_ids = []\n",
    "        self.organ_ids = []\n",
    "        self.data_num = -1\n",
    "        self.label_num = -1\n",
    "        self.split_info = {\n",
    "            \"train\": {\n",
    "                \"data\": 0,\n",
    "                \"label\": 0\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"data\": 0,\n",
    "                \"label\": 0\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"data\": 0,\n",
    "                \"label\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def export_json(self):\n",
    "        json_data = {}\n",
    "        for attr in dir(self):\n",
    "            if attr.startswith('__') or attr.startswith('_'):\n",
    "                continue\n",
    "            attr_value = getattr(self, attr)\n",
    "            if callable(attr_value):\n",
    "                continue\n",
    "            json_data[attr] = attr_value\n",
    "        return json_data\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return json.dumps(self.export_json(), indent=4, ensure_ascii=False)\n",
    "\n",
    "    def merge_json(self, json: dict):\n",
    "        for attr in dir(self):\n",
    "            if attr.startswith('__') or attr.startswith('_'):\n",
    "                continue\n",
    "            attr_value = getattr(self, attr)\n",
    "            if callable(attr_value):\n",
    "                continue\n",
    "            \n",
    "            if attr in json:\n",
    "                setattr(self, attr, json[attr])\n",
    "\n",
    "mdata_meta = MedicalDataMeta(-1, 'test')\n",
    "mdata_meta.merge_json({\n",
    "    'description': 'hello world'\n",
    "})\n",
    "print(mdata_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 51, 'name': 'ASOCA', 'origin_url': 'https://xxx.xxx', 'description': 'xxxx', 'release_date': '2024.05.21', 'task_ids': [10], 'modality_ids': [0, 1], 'organ_ids': [0, 1], 'data_num': 114514, 'label_num': 0, 'split_info': {'train': {'data': 110, 'label': 110}, 'test': {'data': 100, 'label': 100}, 'val': {'data': 0, 'label': 0}}}\n"
     ]
    }
   ],
   "source": [
    "from erine import ask_llm\n",
    "\n",
    "def extract_from_dataset_json(dataset_json):\n",
    "    res = ask_llm([\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"我现在需要从 dataset.json 中提炼出我想要的关于一个数据的信息，将数据整合成下面这样的形式：\n",
    "{\n",
    "    \"id\": 51,\n",
    "    \"name\": \"ASOCA\",\n",
    "    \"origin_url\": \"https://xxx.xxx\",\n",
    "    \"description\": \"xxxx\",\n",
    "    \"release_date\": \"2024.05.21\",\n",
    "    \"task_ids\": [\n",
    "        10\n",
    "    ],\n",
    "    \"modality_ids\": [\n",
    "        0,\n",
    "        1\n",
    "    ],\n",
    "    \"organ_ids\": [\n",
    "        0,\n",
    "        1\n",
    "    ],\n",
    "    \"data_num\": 114514,\n",
    "    \"label_num\": 0,\n",
    "    \"split_info\": {\n",
    "        \"train\": {\n",
    "            \"data\": 110,\n",
    "            \"label\": 110\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"data\": 100,\n",
    "            \"label\": 100\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"data\": 0,\n",
    "            \"label\": 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "如果拿到的数据不完全满足条件，使用上述的默认值即可，但是一定要返回满足上面要求的 json. 不要返回任何注释和填充符号。\n",
    "\n",
    "ONLY RETURN JSON FORMAT WITHOUT ANY OTHER WORDS\n",
    "\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"好的，我会按照你的要求只整理出json\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": str(dataset_json)\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    left, right = -1, -1\n",
    "    stack = []\n",
    "    for i, ch in enumerate(res):\n",
    "        if ch == '{':\n",
    "            if len(stack) == 0:\n",
    "                left = i\n",
    "            stack.append(ch)\n",
    "        elif ch == '}':\n",
    "            stack.pop()\n",
    "            if len(stack) == 0:\n",
    "                right = i + 1\n",
    "                break\n",
    "    \n",
    "    json_str = res[left: right]\n",
    "    return eval(json_str)\n",
    "    \n",
    "\n",
    "res = extract_from_dataset_json({\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"background\": 0,\n",
    "        \"Liver\": 1\n",
    "    },\n",
    "    \"numTraining\": 20,\n",
    "    \"file_ending\": \".nii.gz\",\n",
    "    \"name\": \"Sliver07\",\n",
    "    \"reference\": \"none\",\n",
    "    \"release\": \"prerelease\",\n",
    "    \"description\": \"Sliver07\",\n",
    "    \"overwrite_image_reader_writer\": \"NibabelIOWithReorient\"\n",
    "})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_multitime_llm(json: str):\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            res = extract_from_dataset_json(json)\n",
    "            return res\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def explore_dataset(dataset: str, mdata_meta: MedicalDataMeta):\n",
    "    for file in os.listdir(dataset):\n",
    "        filepath = os.path.join(dataset, file)\n",
    "        if file == 'dataset.json':\n",
    "            dataset_json: dict = json.load(open(filepath, 'r'))\n",
    "            llm_json = try_multitime_llm(dataset_json)\n",
    "            if llm_json:\n",
    "                mdata_meta.merge_json(llm_json)\n",
    "            \n",
    "            mdata_meta.name = dataset_json.get('name', mdata_meta.name)\n",
    "            mdata_meta.orgin_url = dataset_json.get('reference', mdata_meta.orgin_url)\n",
    "            mdata_meta.description = dataset_json.get('description', mdata_meta.description)\n",
    "            \n",
    "data = { 'storage': [] }\n",
    "for file in os.listdir(data_root):\n",
    "    file_path = os.path.join(data_root, file)\n",
    "    data_id, data_name = file.split('_')\n",
    "    mdata_meta = MedicalDataMeta(int(data_id), data_name)\n",
    "    explore_dataset(file_path, mdata_meta)\n",
    "    mdata_meta.id = int(data_id)\n",
    "    data['storage'].append(mdata_meta.export_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_lung_020.nii.gz', '_colon_042.nii.gz', '_hepaticvessel_284.nii.gz', 'nii', 'nii.gz', 'jpg'}\n"
     ]
    }
   ],
   "source": [
    "# 看一下所有 imagesTr 下的文件名后缀\n",
    "datamapper: dict[int, MedicalDataMeta] = {}\n",
    "for mdata in data['storage']:\n",
    "    datamapper[mdata['id']] = mdata\n",
    "\n",
    "suffix_set = set()\n",
    "\n",
    "def get_suffix(path):\n",
    "    if '.' in path:\n",
    "        sample_suffix = '.'.join(path.split('.')[1:])\n",
    "        return sample_suffix\n",
    "    else:\n",
    "        return 'unknown'\n",
    "    \n",
    "def explore_dataset(dataset: str, mdata_meta: MedicalDataMeta):\n",
    "    modality_set = set(mdata_meta)\n",
    "    for file in os.listdir(dataset):\n",
    "        filepath = os.path.join(dataset, file)\n",
    "        if file == 'imagesTr':\n",
    "            sample_num = len(os.listdir(filepath))\n",
    "            sample_file = os.listdir(filepath)[0]\n",
    "            suffix_set.add(get_suffix(sample_file))\n",
    "            \n",
    "        elif file == 'imagesVal':\n",
    "            sample_num = len(os.listdir(filepath))\n",
    "            sample_file = os.listdir(filepath)[0]\n",
    "            suffix_set.add(get_suffix(sample_file))\n",
    "        elif file == 'labelsTr':\n",
    "            sample_num = len(os.listdir(filepath))\n",
    "            sample_file = os.listdir(filepath)[0]\n",
    "            suffix_set.add(get_suffix(sample_file))\n",
    "\n",
    "for file in os.listdir(data_root):\n",
    "    file_path = os.path.join(data_root, file)\n",
    "    data_id, data_name = file.split('_')\n",
    "    mdata_meta = datamapper[int(data_id)]\n",
    "    explore_dataset(file_path, mdata_meta)\n",
    "\n",
    "print(suffix_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamapper: dict[int, dict] = {}\n",
    "for mdata in data['storage']:\n",
    "    datamapper[mdata['id']] = mdata\n",
    "\n",
    "def get_filetype_by_suffix(path: str):\n",
    "    if '/' in path:\n",
    "        path = path.split('/')[-1]\n",
    "    \n",
    "    if path.endswith('.nii.gz') or path.endswith('.nii'):\n",
    "        return modality_name2id['ct']\n",
    "\n",
    "def explore_dataset(dataset: str, mdata_meta: dict):\n",
    "    # modality_set = set(mdata_meta)\n",
    "        \n",
    "    for file in os.listdir(dataset):\n",
    "        filepath = os.path.join(dataset, file)\n",
    "        if file == 'imagesTr':\n",
    "            sample_num = len(os.listdir(filepath))\n",
    "            mdata_meta['split_info']['train']['data'] = sample_num\n",
    "            \n",
    "        elif file == 'imagesVal':\n",
    "            sample_num = len(os.listdir(filepath))\n",
    "            mdata_meta['split_info']['val']['data'] = sample_num\n",
    "            \n",
    "        elif file == 'labelsTr':\n",
    "            sample_num = len(os.listdir(filepath))\n",
    "            mdata_meta['split_info']['train']['label'] = sample_num\n",
    "            \n",
    "        elif file == 'labelsVal':\n",
    "            sample_num = len(os.listdir(filepath))\n",
    "            mdata_meta['split_info']['val']['label'] = sample_num\n",
    "            \n",
    "for file in os.listdir(data_root):\n",
    "    file_path = os.path.join(data_root, file)\n",
    "    data_id, data_name = file.split('_')\n",
    "    mdata_meta = datamapper[int(data_id)]\n",
    "    explore_dataset(file_path, mdata_meta)\n",
    "    mdata_meta['data_num'] = mdata_meta['split_info']['train']['data'] + \\\n",
    "                          mdata_meta['split_info']['test']['data'] + \\\n",
    "                          mdata_meta['split_info']['val']['data']\n",
    "    \n",
    "    mdata_meta['label_num'] = mdata_meta['split_info']['train']['label'] + \\\n",
    "                          mdata_meta['split_info']['test']['label'] + \\\n",
    "                          mdata_meta['split_info']['val']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/storage.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(data, fp, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如果运行过之前的，请从当前开始运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行第二次迭代\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "with open('../data/storage.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(data, fp, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
